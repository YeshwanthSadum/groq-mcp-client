{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a330a2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "# from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # load environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a999c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_FILE_PATH = \"/home/dell/Documents/mcp/weather/weather.py\"\n",
    "SERVER_FILE_PATH_1 = \"/home/dell/Documents/mcp/weather/weather-cpy.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d56d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        # Initialize exit stack and dictionary to hold sessions keyed by server identifier (e.g., script path)\n",
    "        self.sessions: Dict[str, ClientSession] = {}\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "\n",
    "    async def connect_to_server(self, server_id: str, server_script_path: str):\n",
    "        \"\"\"Connect to an MCP server and store the session using a custom server ID\n",
    "\n",
    "        Args:\n",
    "            server_id: Unique identifier for the server (e.g., script path or custom name)\n",
    "            server_script_path: Path to the server script (.py or .js)\n",
    "        \"\"\"\n",
    "        is_python = server_script_path.endswith(\".py\")\n",
    "        is_js = server_script_path.endswith(\".js\")\n",
    "        if not (is_python or is_js):\n",
    "            raise ValueError(\"Server script must be a .py or .js file\")\n",
    "\n",
    "        command = \"python\" if is_python else \"node\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command, args=[server_script_path], env=None\n",
    "        )\n",
    "\n",
    "        # Set up the transport for connecting to the server\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(\n",
    "            stdio_client(server_params)\n",
    "        )\n",
    "        stdio, write = stdio_transport\n",
    "        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))\n",
    "\n",
    "        # Initialize the session\n",
    "        await session.initialize()\n",
    "\n",
    "        # List available tools from the server\n",
    "        response = await session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(\n",
    "            f\"\\nConnected to server with ID {server_id} with tools:\",\n",
    "            [tool.name for tool in tools],\n",
    "        )\n",
    "\n",
    "        # Store the session in the dictionary using the server_id\n",
    "        self.sessions[server_id] = session\n",
    "\n",
    "    async def connect_to_sse_server(self, server_id: str, server_url: str):\n",
    "        \"\"\"Connect to an MCP server running with SSE transport\"\"\"\n",
    "        # Store the context managers so they stay alive\n",
    "        # Set up the transport for connecting to the server\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(\n",
    "            sse_client(url=server_url)\n",
    "        )\n",
    "        stdio, write = stdio_transport\n",
    "        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))\n",
    "\n",
    "        # Initialize\n",
    "        await session.initialize()\n",
    "\n",
    "        # List available tools from the server\n",
    "        response = await session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(\n",
    "            f\"\\nConnected to server with ID {server_id} with tools:\",\n",
    "            [tool.name for tool in tools],\n",
    "        )\n",
    "\n",
    "        # Store the session in the dictionary using the server_id\n",
    "        self.sessions[server_id] = session\n",
    "\n",
    "        # Update the available tools and servers\n",
    "        # await self._get_available_tools()\n",
    "\n",
    "    async def interact_with_server(self, server_id: str, action: str):\n",
    "        \"\"\"Interact with a specific server using its ID\n",
    "\n",
    "        Args:\n",
    "            server_id: Unique identifier for the server\n",
    "            action: The action you want to perform with the selected server\n",
    "        \"\"\"\n",
    "        if server_id not in self.sessions:\n",
    "            raise ValueError(f\"Server with ID {server_id} not found\")\n",
    "\n",
    "        session = self.sessions[server_id]\n",
    "\n",
    "        # Example action: Listing tools (can be extended for other actions)\n",
    "        if action == \"list_tools\":\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(\n",
    "                f\"Tools available on server {server_id}:\", [tool.name for tool in tools]\n",
    "            )\n",
    "\n",
    "    async def close_all_connections(self):\n",
    "        \"\"\"Close all active server connections.\"\"\"\n",
    "        for server_id, session in self.sessions.items():\n",
    "            await session.closeGracefully()\n",
    "            print(f\"Closed connection for server {server_id}\")\n",
    "\n",
    "        # Ensure all resources are properly cleaned up\n",
    "        await self.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to server with ID server1 with tools: ['get_alerts', 'get_forecast']\n",
      "Tools available on server server1: ['get_alerts', 'get_forecast']\n",
      "<contextlib.AsyncExitStack object at 0x7f1582a6c6d0>\n"
     ]
    }
   ],
   "source": [
    "client = MCPClient()\n",
    "\n",
    "# Connect to multiple servers with unique server_ids\n",
    "await client.connect_to_server(\"server1\", SERVER_FILE_PATH)  # nnect to first server\n",
    "# await client.connect_to_server(\"server2\", SERVER_FILE_PATH_1)\n",
    "\n",
    "# Interact with a specific server by its unique ID (e.g., server1)\n",
    "await client.interact_with_server(\"server1\", \"list_tools\")\n",
    "# await client.interact_with_server(\"server2\", \"list_tools\")\n",
    "\n",
    "await client.interact_with_server(\"sse1\", \"https://btc.maratech.com/sse\")\n",
    "\n",
    "print(client.exit_stack)\n",
    "\n",
    "# Optionally, close all connections when done\n",
    "# await client.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2ba35687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await client.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "31dd821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "groq_client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d0858647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You anser questions related to wheter using the available tools. Use bullet points'}, {'role': 'user', 'content': 'what is the weather alert in the largest US state'}]\n",
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=[ChatCompletionMessageToolCall(id='call_01ae', function=Function(arguments='{\"state\": \"AK\"}', name='get_alerts'), type='function')]))\n",
      "{'server1': ['get_alerts', 'get_forecast']}\n",
      "tool_calls\n",
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=[ChatCompletionMessageToolCall(id='call_01ae', function=Function(arguments='{\"state\": \"AK\"}', name='get_alerts'), type='function')]))\n",
      "tool={'id': 'call_01ae', 'function': {'arguments': '{\"state\": \"AK\"}', 'name': 'get_alerts'}, 'type': 'function'}\n",
      "get_alerts <class 'str'>\n",
      "\n",
      "Event: Special Weather Statement\n",
      "Area: Southern Seward Peninsula Coast; Interior Seward Peninsula; Eastern Norton Sound and Nulato Hills; Yukon Delta Coast; Lower Yukon River; St Lawrence Island; Middle Yukon Valley; Lower Yukon and Innoko Valleys\n",
      "Severity: Moderate\n",
      "Description: A band of rain/snow showers is stretching across the Yukon Delta,\n",
      "to St. Lawrence Island and up to the southern Seward Peninsula\n",
      "through tonight. Showers are expected to be a messy rain/snow\n",
      "mix, especially in the Yukon Delta and lower Yukon Valley where\n",
      "the mix will be more rain than snow. Snow becomes more dominant\n",
      "for areas from Shaktoolik north, but showers will be less frequent\n",
      "there keeping accumulations light. Higher snow accumulations of 1\n",
      "to 3 inches in the Nulato Hills where much more of the\n",
      "precipitation will fall as snow. 2 to 3 inches of snow are\n",
      "expected for St. Lawrence Island where showers that are mostly\n",
      "snow will last into Tuesday night.\n",
      "Instructions: None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You anser questions related to wheter using the available tools. Use bullet points'},\n",
       " {'role': 'user',\n",
       "  'content': 'what is the weather alert in the largest US state'},\n",
       " {'role': 'assistant',\n",
       "  'content': '[{\\'id\\': \\'call_01ae\\', \\'function\\': {\\'arguments\\': \\'{\"state\": \"AK\"}\\', \\'name\\': \\'get_alerts\\'}, \\'type\\': \\'function\\'}]'},\n",
       " {'role': 'tool',\n",
       "  'content': \"[{'type': 'text', 'text': '\\\\nEvent: Special Weather Statement\\\\nArea: Southern Seward Peninsula Coast; Interior Seward Peninsula; Eastern Norton Sound and Nulato Hills; Yukon Delta Coast; Lower Yukon River; St Lawrence Island; Middle Yukon Valley; Lower Yukon and Innoko Valleys\\\\nSeverity: Moderate\\\\nDescription: A band of rain/snow showers is stretching across the Yukon Delta,\\\\nto St. Lawrence Island and up to the southern Seward Peninsula\\\\nthrough tonight. Showers are expected to be a messy rain/snow\\\\nmix, especially in the Yukon Delta and lower Yukon Valley where\\\\nthe mix will be more rain than snow. Snow becomes more dominant\\\\nfor areas from Shaktoolik north, but showers will be less frequent\\\\nthere keeping accumulations light. Higher snow accumulations of 1\\\\nto 3 inches in the Nulato Hills where much more of the\\\\nprecipitation will fall as snow. 2 to 3 inches of snow are\\\\nexpected for St. Lawrence Island where showers that are mostly\\\\nsnow will last into Tuesday night.\\\\nInstructions: None\\\\n', 'annotations': None}]\",\n",
       "  'tool_call_id': 'call_01ae'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'There is a Special Weather Statement for the largest US state, Alaska. The main points are:\\n\\n* A band of rain/snow showers is expected to stretch across the Yukon Delta, St. Lawrence Island, and the southern Seward Peninsula through tonight.\\n* Showers will be a messy rain/snow mix, especially in the Yukon Delta and lower Yukon Valley where the mix will be more rain than snow.\\n* Snow will become more dominant for areas from Shaktoolik north, but showers will be less frequent there, keeping accumulations light.\\n* Higher snow accumulations of 1 to 3 inches are expected for the Nulato Hills where much more of the precipitation will fall as snow.\\n* 2 to 3 inches of snow are expected for St. Lawrence Island where showers that are mostly snow will last into Tuesday night.'}]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"llama-3.1-8b-instant\"\n",
    "# MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "async def process_query(messages: list, query: str) -> str:\n",
    "    \"\"\"Process a query using Claude and available tools\"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    available_tools = []\n",
    "    mcp_servers = {}\n",
    "    for server_name, session in client.sessions.items():\n",
    "        response = await session.list_tools()\n",
    "        available_tools.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"parameters\": tool.inputSchema,\n",
    "                    },\n",
    "                }\n",
    "                for tool in response.tools\n",
    "            ]\n",
    "        )\n",
    "        mcp_servers[server_name] = [tool.name for tool in response.tools]\n",
    "\n",
    "    if not mcp_servers:\n",
    "        raise ValueError(\"No MCP servers found\")\n",
    "\n",
    "    print(messages)\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_completion_tokens=4096,\n",
    "    )\n",
    "\n",
    "    choice = response.choices[0]\n",
    "\n",
    "    print(choice)\n",
    "\n",
    "    print(mcp_servers)\n",
    "\n",
    "    print(choice.finish_reason)\n",
    "    print(choice)\n",
    "\n",
    "    if choice.finish_reason == \"tool_calls\":\n",
    "        tool_calls = choice.message.model_dump()[\"tool_calls\"]\n",
    "        # record the tool calls\n",
    "        messages.append({\"role\": \"assistant\", \"content\": str(tool_calls)})\n",
    "\n",
    "        for tool in tool_calls:\n",
    "\n",
    "            print(f\"{tool=}\")\n",
    "            server = None\n",
    "            # find the server that has the selected tool\n",
    "            for server_name, tool_lst in mcp_servers.items():\n",
    "                if tool[\"function\"][\"name\"] in tool_lst:\n",
    "                    server = server_name\n",
    "                    break\n",
    "\n",
    "            if not server:\n",
    "                raise ValueError(\n",
    "                    f\"Server for tool {tool['function']['name']} not found\"\n",
    "                )\n",
    "\n",
    "            print(tool[\"function\"][\"name\"], type(tool[\"function\"][\"arguments\"]))\n",
    "            # tool call\n",
    "            response = await client.sessions[server_name].call_tool(\n",
    "                tool[\"function\"][\"name\"], json.loads(tool[\"function\"][\"arguments\"])\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": str(response.model_dump()[\"content\"]),\n",
    "                    \"tool_call_id\": tool[\"id\"],\n",
    "                }\n",
    "            )\n",
    "            print(response.content[0].text)\n",
    "\n",
    "            # LLM call\n",
    "            response = groq_client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages,\n",
    "                max_completion_tokens=4096,\n",
    "            )\n",
    "            # record assistant response\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "            )\n",
    "    else:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": choice.message.content})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "messages = await process_query(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You anser questions related to wheter using the available tools. Use bullet points\",\n",
    "        }\n",
    "    ],\n",
    "    \"what is the weather alert in the largest US state\",\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f2ab7c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a Special Weather Statement for the largest US state, Alaska. The main points are:\n",
      "\n",
      "* A band of rain/snow showers is expected to stretch across the Yukon Delta, St. Lawrence Island, and the southern Seward Peninsula through tonight.\n",
      "* Showers will be a messy rain/snow mix, especially in the Yukon Delta and lower Yukon Valley where the mix will be more rain than snow.\n",
      "* Snow will become more dominant for areas from Shaktoolik north, but showers will be less frequent there, keeping accumulations light.\n",
      "* Higher snow accumulations of 1 to 3 inches are expected for the Nulato Hills where much more of the precipitation will fall as snow.\n",
      "* 2 to 3 inches of snow are expected for St. Lawrence Island where showers that are mostly snow will last into Tuesday night.\n"
     ]
    }
   ],
   "source": [
    "print(messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88c68d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_sse_server(server_id: str, server_url: str):\n",
    "    \"\"\"Connect to an MCP server running with SSE transport\"\"\"\n",
    "    # Store the context managers so they stay alive\n",
    "    _streams_context = sse_client(url=server_url)\n",
    "    streams = await _streams_context.__aenter__()\n",
    "\n",
    "    _session_context = ClientSession(*streams)\n",
    "    session = await _session_context.__aenter__()\n",
    "\n",
    "    # Initialize\n",
    "    await session.initialize()\n",
    "\n",
    "    # List available tools to verify connection\n",
    "    print(\"Initialized SSE client...\")\n",
    "    print(\"Listing tools...\")\n",
    "    response = await session.list_tools()\n",
    "    tools = response.tools\n",
    "    print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "\n",
    "    res = await session.call_tool(\"get_block_chain_info\", {})\n",
    "    print(res.content[0].text)\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006de28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9ab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5de84e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SSE client...\n",
      "Listing tools...\n",
      "\n",
      "Connected to server with tools: ['get_chain_tips', 'get_block_chain_info', 'get_latest_block_height', 'get_latest_block_hash', 'get_latest_block_info', 'get_fee_info', 'get_difficulty', 'get_mining_stats_info', 'get_mempool_info', 'get_mempool_txids']\n",
      "\n",
      "Chain: main\n",
      "Blocks: 893467\n",
      "Headers: 893467\n",
      "BestBlockHash: 000000000000000000008ff4400965078559f0fcb49aa7f25d92bb282fa8b016\n",
      "Difficulty: 123234387977050.9\n",
      "Time: 1745303346\n",
      "MedianTime: 1745302263\n",
      "VerificationProgress: 0.999994211039964\n",
      "InitialBlockDownload: False\n",
      "Chainwork: 0000000000000000000000000000000000000000bdace24ce56de6e3d4222118\n",
      "SizeOnDisk: 743591297713\n",
      "Pruned: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = await connect_to_sse_server(\"s1\", \"https://btc.maratech.com/sse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "205f4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# chat_completion = groq_client.chat.completions.create(\n",
    "#     #\n",
    "#     # Required parameters\n",
    "#     #\n",
    "#     messages=[\n",
    "#         # Set an optional system message. This sets the behavior of the\n",
    "#         # assistant and can be used to provide specific instructions for\n",
    "#         # how it should behave throughout the conversation.\n",
    "#         {\"role\": \"system\", \"content\": \"you are a helpful assistant.\"},\n",
    "#         # Set a user message for the assistant to respond to.\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain the importance of fast language models\",\n",
    "#         },\n",
    "#     ],\n",
    "#     # The language model which will generate the completion.\n",
    "#     model=\"llama-3.3-70b-versatile\",\n",
    "#     #\n",
    "#     # Optional parameters\n",
    "#     #\n",
    "#     # Controls randomness: lowering results in less random completions.\n",
    "#     # As the temperature approaches zero, the model will become deterministic\n",
    "#     # and repetitive.\n",
    "#     temperature=0.5,\n",
    "#     # The maximum number of tokens to generate. Requests can use up to\n",
    "#     # 32,768 tokens shared between prompt and completion.\n",
    "#     max_completion_tokens=1024,\n",
    "#     # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "#     # likelihood-weighted options are considered.\n",
    "#     top_p=1,\n",
    "#     # A stop sequence is a predefined or user-specified text string that\n",
    "#     # signals an AI to stop generating content, ensuring its responses\n",
    "#     # remain focused and concise. Examples include punctuation marks and\n",
    "#     # markers like \"[end]\".\n",
    "#     stop=None,\n",
    "#     # If set, partial message deltas will be sent.\n",
    "#     stream=False,\n",
    "# )\n",
    "\n",
    "# # Print the completion returned by the LLM.\n",
    "# print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8ab1fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tool use\n",
    "\n",
    "# payload = {\n",
    "#     \"model\": \"llama-3.3-70b-versatile\",\n",
    "#     \"messages\": [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"You are a weather assistant. Use the get_weather function to retrieve weather information for a given location.\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": \"What's the weather like in Mysore?\"},\n",
    "#     ],\n",
    "#     \"tools\": [\n",
    "#         {\n",
    "#             \"type\": \"function\",\n",
    "#             \"function\": {\n",
    "#                 \"name\": \"get_weather\",\n",
    "#                 \"description\": \"Get the current weather for a location\",\n",
    "#                 \"parameters\": {\n",
    "#                     \"type\": \"object\",\n",
    "#                     \"properties\": {\n",
    "#                         \"location\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                             \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "#                         },\n",
    "#                         \"unit\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                             \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "#                             \"description\": \"The unit of temperature to use. Defaults to fahrenheit.\",\n",
    "#                         },\n",
    "#                     },\n",
    "#                     \"required\": [\"location\"],\n",
    "#                 },\n",
    "#             },\n",
    "#         }\n",
    "#     ],\n",
    "#     \"tool_choice\": \"auto\",\n",
    "#     \"max_completion_tokens\": 4096,\n",
    "# }\n",
    "\n",
    "\n",
    "# response_1 = groq_client.chat.completions.create(**payload)\n",
    "\n",
    "# response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c0360df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0f574ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "250396d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].message.model_dump()[\"tool_calls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "73419bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].message.model_dump()[\"tool_calls\"][0][\"function\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "689dc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].message.tool_calls[0].function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "17d7b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_1.choices[0].message.tool_calls[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1f6a5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_2.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8652194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4db42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = await client.sessions[\"server1\"].list_tools()\n",
    "# available_tools = [\n",
    "#     {\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": tool.name,\n",
    "#             \"description\": tool.description,\n",
    "#             \"parameters\": tool.inputSchema,\n",
    "#         },\n",
    "#     }\n",
    "#     for tool in response.tools\n",
    "# ]\n",
    "\n",
    "# available_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6066c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make the tool call\n",
    "\n",
    "# res = await client.sessions[\"server1\"].call_tool(\"get_alerts\", {\"state\": \"NY\"})\n",
    "# print(res.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5217b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8213a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609bf31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22149710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f93470f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You anser questions related to wheter using the available tools'},\n",
       " {'role': 'user',\n",
       "  'content': 'what is the weather alert in the largest US state'},\n",
       " {'role': 'assistant',\n",
       "  'content': '[{\\'id\\': \\'call_97a7\\', \\'function\\': {\\'arguments\\': \\'{\"state\": \"AK\"}\\', \\'name\\': \\'get_alerts\\'}, \\'type\\': \\'function\\'}]'},\n",
       " {'role': 'tool',\n",
       "  'content': \"[{'type': 'text', 'text': '\\\\nEvent: Special Weather Statement\\\\nArea: Southern Seward Peninsula Coast; Interior Seward Peninsula; Eastern Norton Sound and Nulato Hills; Yukon Delta Coast; Lower Yukon River; St Lawrence Island; Middle Yukon Valley; Lower Yukon and Innoko Valleys\\\\nSeverity: Moderate\\\\nDescription: A band of rain/snow showers is stretching across the Yukon Delta,\\\\nto St. Lawrence Island and up to the southern Seward Peninsula\\\\nthrough tonight. Showers are expected to be a messy rain/snow\\\\nmix, especially in the Yukon Delta and lower Yukon Valley where\\\\nthe mix will be more rain than snow. Snow becomes more dominant\\\\nfor areas from Shaktoolik north, but showers will be less frequent\\\\nthere keeping accumulations light. Higher snow accumulations of 1\\\\nto 3 inches in the Nulato Hills where much more of the\\\\nprecipitation will fall as snow. 2 to 3 inches of snow are\\\\nexpected for St. Lawrence Island where showers that are mostly\\\\nsnow will last into Tuesday night.\\\\nInstructions: None\\\\n', 'annotations': None}]\",\n",
       "  'tool_call_id': 'call_97a7'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on the information available, it appears that there is a special weather statement issued for the southern Seward Peninsula coast, interior Seward Peninsula, eastern Norton Sound and Nulato Hills, Yukon Delta coast, lower Yukon River, St. Lawrence Island, middle Yukon Valley, and lower Yukon and Innoko valleys.\\n\\nThe severity of the weather is classified as moderate, and the description indicates a band of rain/snow showers stretching across the affected areas, with showers expected to be a messy rain/snow mix. The expected precipitation includes light snowfall and 1-3 inches of snow in the Nulato Hills.\\n\\nThere are no instructions provided in the available information, but it is recommended to stay informed and monitor the weather forecast for updates.'}]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4e6ef0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# from typing import Optional\n",
    "# from contextlib import AsyncExitStack\n",
    "\n",
    "# from mcp import ClientSession, StdioServerParameters\n",
    "# from mcp.client.stdio import stdio_client\n",
    "\n",
    "# from anthropic import Anthropic\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()  # load environment variables from .env\n",
    "\n",
    "# class MCPClient:\n",
    "#     def __init__(self):\n",
    "#         # Initialize session and client objects\n",
    "#         self.session: Optional[ClientSession] = None\n",
    "#         self.exit_stack = AsyncExitStack()\n",
    "#         # self.anthropic = Anthropic()\n",
    "\n",
    "#     async def connect_to_server(self, server_script_path: str):\n",
    "#         \"\"\"Connect to an MCP server\n",
    "\n",
    "#         Args:\n",
    "#             server_script_path: Path to the server script (.py or .js)\n",
    "#         \"\"\"\n",
    "#         is_python = server_script_path.endswith('.py')\n",
    "#         is_js = server_script_path.endswith('.js')\n",
    "#         if not (is_python or is_js):\n",
    "#             raise ValueError(\"Server script must be a .py or .js file\")\n",
    "\n",
    "#         command = \"python\" if is_python else \"node\"\n",
    "#         server_params = StdioServerParameters(\n",
    "#             command=command,\n",
    "#             args=[server_script_path],\n",
    "#             env=None\n",
    "#         )\n",
    "\n",
    "#         stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "#         self.stdio, self.write = stdio_transport\n",
    "#         self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "\n",
    "#         await self.session.initialize()\n",
    "\n",
    "#         # List available tools\n",
    "#         response = await self.session.list_tools()\n",
    "#         tools = response.tools\n",
    "#         print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "bac8b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = MCPClient()\n",
    "\n",
    "# # Connect to multiple servers with unique server_ids\n",
    "# await client.connect_to_server(SERVER_FILE_PATH)\n",
    "# await client.connect_to_server(SERVER_FILE_PATH_1) # nnect to second server\n",
    "\n",
    "# # Interact with a specific server by its unique ID (e.g., server1)\n",
    "# # await client.session.list_tools()\n",
    "# response = await client.session.list_tools()\n",
    "# tools = response.tools\n",
    "# print(\n",
    "#     f\"Tools available on server:\", [tool.name for tool in tools]\n",
    "# )\n",
    "\n",
    "# # print(client.exit_stack)\n",
    "\n",
    "# # Optionally, close all connections when done\n",
    "# # await client.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb03503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5e40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc456e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
